% vi:tw=72:fenc=utf-8
\documentclass{../GPUSPHtemplate}

\title{GPUSPH Installation Guide}

\author{}

\date{\currentver\ --- \reldate}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}

GPUSPH is an implementation of Smoothed Particle Hydrodynamics (SPH) on
\nvidia\ CUDA-enabled graphics cards. The first version of GPUSPH was
developed by Alexis Hérault, guided by SPHysics, and presented at the
Third SPHERIC Workshop in Lausanne, Switzerland in 2008. 
The graphics processing unit (GPU) implementation came from GPU-LAVA, 
a lava flow program, developed by Hérault and Bilotta at INGV in 
Catania, Italy. The present version of GPUSPH is open source, 
licensed under the GNU General Public License
(\url{www.gnu.org/licenses/gpl.txt}).

Smoothed Particle Hydrodynamics (SPH) is a Lagrangian meshless numerical
method that was developed in astrophysics by \cite{lucy_numerical_1977} and
\cite{gingold_smoothed_1977}. Its first application to free surface flows (e.g.
dam breaks and waves) was by \cite{monaghan_volcanoes_1994}.
% \cite{gomez-gesteira_using_2004} and \cite{dalrymple_numerical_2006}, also
% applying SPH to dam breaks and waves, began the development of SPHysics,
% an open source FORTRAN code (\url{http://www.sphysics.org}),
% \cite{gomez-gesteira_sphysics_2012}.
Since in SPH the interactions between particles involve many neighbors
(several hundreds in three dimensions), it suffers from high computational costs.
This motivated the development of massively parallel SPH codes,
in particular codes running on graphics cards due to their performance and relatively
low cost.

The development of sophisticated graphics cards is driven by the demands of 
advanced computer gaming, in particular to handle three-dimensional 
graphics for the computer display. Each of these
graphics cards has numerous streaming processors to do the mathematics
of image rotation, resizing etc. With the advent of the {\em CUDA}
programing language from \nvidia\ in 2007, simple \cpp\ language can be used
to access the mathematical power of these massively parallel cards. For
computer simulations that are not data-intensive, GPU programming
provides supercomputer capabilities at commodity prices.

Some timing information can be found in \cite{herault_sph_2010}, 
showing that using the GPU is far faster (orders
of magnitude) than using a CPU to compute SPH models. Speedups of 100
can be achieved for parts of the code when compared to serial versions
of the code.

The first version of GPUSPH was running  on \nvidia's Compute Capability (CC) 1.x cards, 
GeForce 8xxx cards. From this first version we tested GPUSPH on all \nvidia\  architectures
(from Fermi CC 2.x to the latest Pascal CC 6.x). \nvidia\  dropped support for CC 1.x and will 
soon do it for CC 2.x. So at the moment GPUSPH will run on any card with CC 2.x or higher
(from Fermi up) but we expect to drop soon the support for CC 2.x. When done GPUSPH will
run on any card with CC 3.x or higher (from Kelpler up).

This guide is divided into several sections. 
First, the installation and set-up of the GPUSPH code is explained and 
some example problems to illustrate its use are provided.
The second chapter goes through all the steps necessary to build a new
simulation and post-process the results.
The third chapter deals with an overview of SPH, with which the reader should
have some familiarity. 
Finally we discuss the nature of the GPUSPH program in some detail.

\section{Installation of GPUSPH}

The first step to run GPUSPH is to install the \nvidia\ company's CUDA
compilers and libraries (directions given below). CUDA is an extension
of the \cpp\ language to allow \cpp\ to talk to the graphics card.

The second step is to install the open source software, CHRONO,
which simulates rigid body dynamics. This library is used for
any rigid objects that move, such as floating objects or objects moved
by fluid flow.

The third step is to obtain, compile and run GPUSPH.

\textbf{Remark:} to run multi-node simulations, you also need to
install OpenMP (>= 1.8.4).

\subsection{Installing CUDA}

Ensure that your computer has an \nvidia\ graphics card that is CUDA
enabled. The \nvidia\ website has a list of all the CUDA-enabled graphics
cards: \url{www.nvidia.com/object/cuda_gpus.html}. You can check whether
CUDA is already installed on your machine by launching the command:
\begin{shellcode}
nvidia-smi
\end{shellcode}
from the terminal. If CUDA is installed it will give you information
on the current state of the NVIDIA graphics card(s) on the machine.

\textbf{Note}: GPUSPH runs on cards with Compute Capabillity at least 2.0


\textbf{Remark}: regarding the choice of the GPU, the more CUDA 
cores and the more memory on the card, the better. Anyway a
mid range mobile GPU's like GT750m/GT840 with 1 or 2GB of memory
is sufficient to run significant simulations. A laptop with such a GPU will be
a perfect mobile developing and testing platform.

The GPU programming language CUDA can be obtained from the \nvidia\ website,
CUDA Zone. The CUDA Toolkit and CUDA Software Development Kit (SDK)
need to be installed for your operating system along with the video
driver. These packages include the CUDA compiler \cmd{nvcc}, which is
needed to develop executable code, and the graphics card driver that
allows your program to access the GPU card.

Download the relevant driver for your machine from:
\url{http://www.nvidia.com/Download/index.aspx?lang=en-us}
and the CUDA toolkit from:
\url{https://developer.nvidia.com/cuda-downloads}
Follow the instructions provided by \nvidia\ for the installation.


To ensure that all is installed correctly and working, you should
compile and run the SDK examples, which include many programs that
illustrate the capabilities of CUDA and the GPU; for example, \nvidia's
sorting program \cmd{radixSort} is used by GPUSPH to organize the
neighbor list. Some interesting SDK programs are \cmd{fluidsGL} and
\cmd{particles}. To compile the SDK programs, after the SDK is
installed, go to \url{/Developer/GPU Computing/C} and (on a unix/linux
or mac machine), type \cmd{make} on a terminal window command line. This
should create a directory of executable examples located within the C
directory called bin/darwin/release for the mac and bin/linux/release
for a linux machine. In this directory, type \cmd{./fluidsGL} to run
the \cmd{fluidsGL} example. You should see a green window open on your
desktop. Use the mouse to stir up the fluid. The example program
Particles is worth playing with as well, as it provided a basis for
developing GPUSPH.



\subsection{Installing GPUSPH}

The GPUSPH source code is hosted on \href{http://github.com}{GitHub}.
The project's GitHub page is \url{http://github.com/GPUSPH/gpusph}.

To obtain the GPUSPH code, you can either use the \cmd{git} revision
control system, or download a \cmd{.zip}ped archive of a specific
version. This manual refers to \currentver\ of GPUSPH.

If you have \cmd{git} installed, you can use
\begin{shellcode}[escapeinside=\{\}]
git clone https://github.com/GPUSPH/gpusph.git
cd gpusph
git checkout v{\version}
\end{shellcode}
to get \currentver\ specifically. Otherwise, download the \cmd{.zip}ped
archive from \url{http://github.com/GPUSPH/gpusph/archive/v\version.zip},
and then
\begin{shellcode}[escapeinside=\{\}]
unzip v{\version}.zip
cd gpusph-{\version}
\end{shellcode}
(you may remove \cmd{v}\version\cmd{.zip} afterwards).

Within the top directory, you can find the \cmd{Makefile}, a \cmd{src}
directory (holding the main GPUSPH source), a \cmd{scripts} directory
(holding various auxiliary scripts), a copy of the license, settings to
produce internal documentation with Doxygen, and a sample Digital
Elevation Model (DEM) data file.

The most interesting source files in \cmd{src} are the \cmd{Problem}s.
A few sample problems are shipped with GPUSPH, showing how to employ
specific features. You can get a list of the available problems by
running
\begin{shellcode}
make list-problems
\end{shellcode}

To build and test GPUSPH, you can run
\begin{shellcode}
make test
\end{shellcode}
which should automatically detect your configuration, such as the
compute capability of your GPU as well as the availability of optional
libraries such as MPI (for mulit-node support) or HDF5 (to read HDF5SPH
data files).

When the building completes, you will have some new directoryes
(\cmd{build} and \cmd{dist}) and a \cmd{GPUSPH} soft link to the
compiled binary. \cmd{make test} will also automatically run
\cmd{./GPUSPH} for you.

After building, simply runnning \cmd{./GPUSPH} will run the program
again.

\subsection{Installing the CHRONO library}

The CHRONO website provides information for how to install CHRONO:
\url{http://api.chrono.projectchrono.org/tutorial_install_chrono.html}

\textbf{Remark:} There is no need for the Irrlicht library with GPUSPH.

In this section we summarize the steps for the CHRONO library installation.
To install CHRONO, besides the GPUSPH requirements you need to have \cmd{cmake} installed
and a cmake interface like \cmd{ccmake} on Linux.

First, create a directory where to install CHRONO:
\begin{shellcode}
mkdir install_chrono
\end{shellcode}
In that directory, clone the CHRONO repository from Github in a source directory:
\begin{shellcode}
cd install_chrono
git clone https://github.com/projectchrono/chrono.git source
\end{shellcode}
This command will download the CHRONO repository in a folder named \cmd{source}.

Create a folder where to build CHRONO:
\begin{shellcode}
mkdir build
\end{shellcode}
From the \cmd{build} repository, run \cmd{cmake}:
\begin{shellcode}
cmake ../source
\end{shellcode}
Configure the compilation options with \cmd{ccmake}:
\begin{shellcode}
ccmake .
\end{shellcode}
set the following options to off:
\begin{shellcode}
 ENABLE_MODULE_CASCADE            OFF
 ENABLE_MODULE_COSIMULATION       OFF
 ENABLE_MODULE_FEA                OFF
 ENABLE_MODULE_FSI                OFF
 ENABLE_MODULE_IRRLICHT           OFF
 ENABLE_MODULE_MATLAB             OFF
 ENABLE_MODULE_MKL                OFF
 ENABLE_MODULE_OPENGL             OFF
 ENABLE_MODULE_PARALLEL           OFF
 ENABLE_MODULE_POSTPROCESS        OFF
 ENABLE_MODULE_PYTHON             OFF
 ENABLE_MODULE_VEHICLE            OFF
 ENABLE_OPENMP                    OFF
\end{shellcode}

Once this is done, you can compile the CHRONO project
(still from the \cmd{build} folder):
\begin{shellcode}
make
\end{shellcode}
and install the library (also from the \cmd{build} folder):
\begin{shellcode}
sudo make install
\end{shellcode}

%\todo{How to install relevant packages on common distributions
%(Debian/Ubuntu, Arch, Fedora/RedHat).}


\section{Choosing the GPUSPH \cmd{Problem} and other compilation options}\label{sec:compileoptions}

You can test a different problem by using:
\begin{shellcode}
make OtherProblem test
\end{shellcode}
where \cmd{OtherProblem} is the name of a different problem. You can get
a list of available problems with \cmd{make list-problems}.

There are a number of other options available. A complete list of the
options and their description can be obtained by running \cmd{make
help-options}. All options (with the exception of \cmd{plain} and
\cmd{echo}) are persistent across compilations, so they can be set once
with \cmd{make option=value}, and subsequent executions of \cmd{make}
will remember the \cmd{value} set.

%\todo{Describe \cmd{make} options}
The \cmd{make} options are listed below:
\begin{itemize}
\item \cmd{target_arch} - if set to 32, force compilation for 32 bit architecture
\item \cmd{problem} - Name of the problem. Since version 5, you can just use the problem name as a target to build that particular test case.
\item \cmd{dbg} - 0 no debugging, 1 enable debugging
\item \cmd{compute} - 11, 12, 13, 20, 21, 30, 35, etc: compute capability to compile for (default: autodetect)
\item \cmd{fastmath} - Enable or disable fastmath. Default: 0 (disabled)
\item \cmd{mpi} - 0 do not use MPI (no multi-node support), 1 use MPI (enable multi-node support). Default: autodetect
\item \cmd{hdf5} - 0 do not use HDF5, 1 use HDF5, 2 use HDF5 and HDF5 requires MPI. Default: autodetect
\item \cmd{verbose} - 0 quiet compiler, 1 ptx assembler, 2 all warnings
\item \cmd{plain} - 0 fancy line-recycling stage announce, 1 plain multi-line stage announce
\item \cmd{echo} - 0 silent, 1 show commands
\item \cmd{chrono} - 0 do not use the CHRONO library, 1 use the CHRONO library
\end{itemize}
To view your current make options type \cmd{make show} instead of make.

\section{Example Problems}

Simulations in GPUSPH are defined in terms of \cmd{Problem}s. Some
example problems are provided with GPUSPH itself, to illustrate the
basics of problem design, and how to use the fundamental building blocks
provided by GPUSPH. Such building blocks include a variety of
geometrical shapes to describe the (fixed) solid boundaries of the
domain, as well as a number of objects that move following prescribed
laws, such as gates, pistons and paddles.

These objects are designed to offer great flexibility in their use, far
beyond what is shown in the sample problems. This flexibility should
allow you to create very complex simulations by combining the objects
appropriately.

The number of particles used in the test problems is deliberately taken
as a small number, simply to allow for fast execution times even on
older hardware. One of the first tests to try is to increase the
resolution by reducing the size of the particles. For example,
by~reducing the particle size from the default of~$0.025$m to the
smalle~$0.02$m, \cmd{DamBreak3D} would run with $21,252$ particles
instead of the default $10,664$.

This can be done in two ways. A permanent change comes about by editing
the problem file (e.g. \cmd{DamBreak3D.cc}) and changing the value
passed as argument of \cmd{set_deltap()} (e.g., replace
\cmd{set_deltap(0.025f);} with \cmd{set_deltap(0.02f);}. The second way
is to specify the particle size at runtime using the appropriate command
line option (described below): e.g. \cmd{./GPUSPH --deltap 0.02}.

\subsection{DamBreak3D}

\cmd{DamBreak3D} is a case originally used by
\cite{gomez-gesteira_using_2004}
for testing a prototype version of SPHysics. It is based on some
experiments done by \cite{arnason_interactions_2005} at the University of Washington.
We assume an instantaneous breaking dam and the resulting flow impinging
onto a rectangular object. The whole problem is contained within a
bounding box, which extends $1.6$m in length ($x$ axis), $0.67$m in
width ($y$ axis), and $0.4$m in height. This is the experimental box.
The fluid behind the dam is a rectangular box of water at one end of the
tank at time equal to zero. The dam is assumed to break instantaneously
so that the column of water, confined on three sides, collapses into the
tank. In the tank there is a vertical rectangular object -- the
collapsing water column impacts on the tank and then flows up the front
face of the object and around the sides. Finally the water hits the back
wall of the tank. A screenshot of the simulation at time $0.6s$ is provided
in the Figure \ref{fig:DamBreak3D}.

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.3, trim={100 100 100 100},clip]{../fig/damBreak3D_screenshot.png}
    \caption{Screenshot of the DamBreak3D simulation at time $0.6s$.}\label{fig:DamBreak3D}   
  \end{center}
\end{figure}

\subsection{DamBreakGate}

In most laboratory experiments of dam breaks, the dam takes a certain
amount of time to move out of the way. The example problem
\cmd{DamBreakGate} illustrates the use of moving boundary particles of
the type GATEPART. The problem is set up the same way as the
\cmd{DamBreak3D} case, but there is a moving gate that is raised
vertically with a linearly varying velocity. In this case, the gate will
move with a velocity that is zero when the problem starts and that
linearly increases with time until the gate is outside the domain. The
effect on the dam break is that the escaping water is affected by the
gage motion. (See \cite{crespo_modeling_2008}'s SPH modeling of
\cite{janosi_turbulent_2004}'s experiment, where a moving gate was important.)

The moving gate is created by defining its geometry with particles
denoted as GATEPART particles and the \cmd{mb_callback} function, which
is used for the \textbf{m}oving \textbf{b}oundaries.

A screenshot of the simulation at time $0.8s$ is provided
in the Figure \ref{fig:DamBreakGate}.

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.35, trim={100 100 100 100},clip]{../fig/damBreakGate_screenshot.png}
    \caption{Screenshot of the DamBreakGate simulation at time $0.8s$.}\label{fig:DamBreakGate}   
  \end{center}
\end{figure}

\subsection{OpenChannel}

This problem represents an instantaneous start up of a highly viscous
and dense fluid flow in an open channel on a $9\deg$ slope. The
channel is rectangular in cross-section ($1$m wide and $0.7$m deep) and
the computed length of the infinitely long channel is $2$m. The side
walls are fixed (Leonard-Jones boundary force) while the computational
ends of the domain are periodic, so that a particle leaving the
downstream end of the model domain enters the upstream end at the same
place, $2$m upstream.

The periodic boundary here is used in the $x$ direction, although
boundaries in other problems can be periodic in the other directions as
well. The key parameter in the problem statement is
\cmd{m_simparams.periodicbound}, which can be set to any combination of
\cmd{PERIODIC_X}, \cmd{PERIODIC_Y}, \cmd{PERIODIC_Z} to indicate
peridocity along each of the axes.
Figure \ref{fig:OpenChannel} shows the shape of the velocity field in 
the channel after time convergence.
\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.35, trim={0 100 0 0},clip]{../fig/openChannel_screenshot.png}
    \caption{Screenshot of the OpenChannel simulation after time convergence.}\label{fig:OpenChannel}
  \end{center}
\end{figure}

\subsection{WaveTank}

WaveTank uses a moving boundary to create a paddle wavemaker at one end
of a wave tank with a sloping bottom (bottom slope is $4.2364\deg$). The
wavemaker motion is controlled by the \cmd{mb_callback} function. In
this case, the length of the paddle is $1.0$m and the paddle pivots
about an origin \cmd{m_origin}; here, the pivot is located $0.1344$m
below the bottom and $0.13$m from the front wall of the tank. To specify
the paddle motion, the angular frequency of the motion ($2 \pi/T$, where
$T=1$s is the wave period), and the wave paddle stroke at the water
surface ($S=0.1$m) are given in the variables \cmd{mb_omega} and
\cmd{mb_amplitude}. To change the stroke and the frequency of the wave
paddle, you must change these variables in the problem file,
\cmd{WaveTank.cc}.
Figure \ref{fig:WaveTank} shows a screenshot of the simulation at time $9s$.

\iffalse
\begin{figure}[h]
\centering{%
\includegraphics[width=0.63\textwidth]{paddle.png}%
}
\caption{Schematic of the wave paddle for \cmd{WaveTank.cc}}
\end{figure}
\else
%\todo{paddle picture}
\fi

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.35, trim={0 200 0 200},clip]{../fig/waveTank_screenshot.png}
    \caption{Screenshot of the WaveTank simulation at time $9s$.}\label{fig:WaveTank}
  \end{center}
\end{figure}

\subsection{SolitaryWave}

SolitaryWave is similar in set up to the WaveTank example, except that a
piston moving boundary is used. The motion of a vertical plate is
determined by the method of \cite{goring_tsunamis_1979}, available in PDF format
from:
\cmd{http://caltechkhr.library.caltech.edu/50/}
The full excursion (stroke) of the paddle is the variable \cmd{S}.

\subsection{Seiche}

The Seiche problem is to examine the influence of shaking on a
rectangular container of size: $\ell = 0.707$m, $w = \ell/2$, and depth,
$H = 0.5$m. The purpose of the example is to illustrate the ability to
vary gravity in a problem. As the problem starts, there is water in the
container. After $0.3$s, gravity is modified by adding a component in
the $x$ direction, such that the total gravity vector is
\cmd{m_physparams.gravity = make_float3(3.*sin(9.8*(t-m_gtstart)), 0.0,
-9.81f);}\\ which means that the container is shaken with a sinusoidal
motion with angular frequency of $9.8s^{-1}$ (period${} = 0.64$s),
with a magnitude of $3\text{m}/\text{s}^2$ until time \cmd{m_gtend=3.0}
is reached, when the gravity vector once again returns to the vertical
acceleration of gravity. After this time, the seiching motion starts to
decrease in amplitude.

\iffalse
\begin{figure}[h]
\centering{%
\includegraphics[scale=0.5]{Seiche.png}%
}
\caption{Resonant seiching in a rectangular domain showing the results
of a time varying gravity in the problem, \cmd{Seiche.cc}. Here the tank has
been shaking side to side at the resonant frequency of $0.638$s. The
color coding is for the pressure in the fluid.}
\end{figure}
\else
%\todo{seiche picture}
\fi

The variation of gravity with time (and any stop (\cmd{m_gtend}) and
start times) is prescribed in a user-supplied (in the problem)
\cmd{g_callback} function.

\subsection{DEMExample}

This is an example showing how to use GPUSPH's support for Digital
Elevation Models (DEMs). It loads the topography of the bottom of the
domain from a file called \cmd{half_wave0.1m.txt}, shipped with GPUSPH.
A different DEM can be used, by either changing the name in the source
\cmd{DEMExample.cu} file, or by providing the new name as argument to the
\cmd{--dem} command-line option to GPUSPH.

%\todo{TestTopo picture}

\section{GPUSPH Command Line Options}\label{options}

When running from the command line, there are several options available
to you to alter some aspects of the GPUSPH run.

\begin{description}
\item [-{}-device \emph{integer}]
Choose which GPU(s) to use for the run.
On the command line: \cmd{./GPUSPH --device N}, where N is the
(integer) number of the device you wish to use. 
To find the number associated with each of your CUDA-enabled 
devices (graphics cards), you
can use the CUDA SDK program DeviceQueryDrv. If you only have one
CUDA-enabled GPU, the only possible choice for N is~0, which is the
default.
If you want to run the simulation on several GPUs, the command is:
\cmd{./GPUSPH --device i,j,k}
\item [-{}-deltap \emph{float}]
Change the resolution (inter-particle spacing) at which the problem
should be run.
\item [-{}-tend \emph{float}]
The model time in seconds when you wish the model to stop.
\item [-{}-dem \emph{string}]
For the Problem DEMExample: the name of the DEM file to use.
\item [-{}-resume \emph{fname}]
Resume from the given file (HotStart file saved by HotWriter).
\item [-{}-checkpoint-every \emph{float}]
HotStart checkpoints will be created every VAL seconds of simulated time (float VAL, 0 disables).
\item [-{}-checkpoints \emph{integer}]
Number of HotStart checkpoints to keep.
\item [-{}-maxiter \emph{integer}]
Break after this many iterations.
\item [-{}-dir \emph{string}]
Use given directory for dumps instead of date-based one.
\item [-{}-nosave]
Disable all file dumps but the last.
\item [-{}-gpudirect]
Enable GPUDirect for RDMA (requires a CUDA-aware MPI library).
\item [-{}-striping]
Enable computation/transfer overlap in multi-GPU (usually convenient for 3+ devices).
\item [-{}-asyncmpi]
Enable asynchronous network transfers (requires GPUDirect and 1 process per device).
\item [-{}-num-hosts \emph{integer}]
Uses multiple processes per node by specifying the number of nodes.
\item [-{}-byslot-scheduling]
MPI scheduler is filling hosts first, as opposite to round robin scheduling.
\item [-{}-debug \emph{flags}]
Enable specified debug flags.
\item [-{}-help]
Show the help and exit.
\end{description}

\section{Running multi-node simulations}

GPUSPH can distribute the computation of a simulation on multiple 
GPU devices attached to different nodes of a cluster in different ways.

Say we want to launch a simulation on $N$ nodes, each with $D$ devices 
(with CUDA device numbers ranging from $0$ to $D-1$); 
the total number of devices in the simulation will be $N \times D$. We can run either:
\begin{itemize}
\item one process per node, $D$ GPUs per process
\item $2$ processes per node, $D/2$ GPUs per process
\item $4$ processes per node, $D/4$ GPUs per process
\item etc.
\end{itemize}

Additionally, some MPI implementations have built-in support for CUDA, 
which allows for faster communication between devices on different nodes. 
Experimental support for this feature can be enabled in GPUSPH with the \cmd{--gpudirect} command-line option.

The best decision on how to distribute the computation across nodes and devices depends on 
the queue policy of the cluster, on the network topology, on simple a posteriori performance tests, 
on the capabilities of the MPI implementation, etc.

If we wanted to run the simulation on all the devices of one node, we would run in an interactive shell:

\begin{shellcode}
./GPUSPH --device 0,1,...D-1
\end{shellcode}

Running the same simulation on multiple nodes only requires to run 
the same command within the reference MPI launcher (usually a script called \cmd{mpirun}); 
GPUSPH will retrieve the necessary information about the launch environment 
directly from the MPI runtime and will organize the node-to-node communication accordingly.

\begin{shellcode}
mpirun -np N ./GPUSPH --device 0,1,...D-1
\end{shellcode}

This command leaves to MPI the choice of which nodes to use in the network, 
if more than N are available. 
It is always safe to provide MPI a list of hostnames corresponding 
to the nodes chosen to run the simulation. 
If the file containing the list of hostnames is called \cmd{myhostsfile}, a typical syntax will be:

\begin{shellcode}
mpirun -np N -hostfile ./myhostsfile \
./GPUSPH --device 0,1,...D-1
\end{shellcode}

Please note the syntax may vary from one MPI library to another. 
For example, MVAPICH uses \cmd{-hostfile} while OpenMPI \cmd{--hostfile}.

Let us now see the command line options needed to run the same simulation 
with more processes (and thus on more nodes) and a smaller number of devices per process. 
In this case we need to inform both the MPI runtime and GPUSPH. 
For the former, we simply decrease the number of processes to start (with the \cmd{-np} option); 
for the latter, we need to shorten appropriately the list of devices 
passed with the \cmd{--device} option. 
If our aim is to run $N*2$ processes each using $D/2$ devices, we then run:
\begin{shellcode}
mpirun -np N*2 -hostfile ./myhostsfile \
./GPUSPH --device 0,1,...D/2-1
\end{shellcode}

Here we need to take care of a few important details. If the list of available 
hosts contains at least $N*2$ hostnames, the MPI runtime will start every process 
on a different physical node. But what happens if the list is shorter 
(e.g. $N$ hosts only), or if we want anyway to use a smaller number of 
nodes (for example because part of the cluster is already used by other processes)? 
The MPI runtime will start multiple processes per node and the GPU device 
numbers will be likely to conflict (i.e. two different processes might 
try to use the same GPU device for different parts of the simulation domain, 
causing a performance slowdown or a failure if the CUDA devices are set
in ``exclusive mode''). 
In this case, we must inform GPUSPH that the number of physical hosts (i.e. nodes) 
is smaller than the number of processes, so that it will shift the GPU device 
numbers of the appropriate processes and no GPU device will be accessed by two processes. 
The corresponding option is \cmd{--num-hosts}:

\begin{shellcode}
mpirun -np N*2 -hostfile ./myhostsfile \
--num-hosts N ./GPUSPH --device 0,1,...D/2-1
\end{shellcode}

But there is another important detail. There are different ways the MPI 
runtime can distribute the processes across the nodes. 
Two very common policies are ``by slot'' (fill-first) and ``by node''
(round robin). 
The scheduling policy affects the association between the process ranks 
and the CUDA device numbers, so GPUSPH must be informed about it to use 
the appropriate offsets. GPUSPH assumes a round robin schedule is being used; 
if this is not true, the \cmd{--byslot-scheduling} option must be passed:

\begin{shellcode}
mpirun -np N*2 -hostfile ./myhostsfile \
--num-hosts N --byslot-scheduling \
./GPUSPH --device 0,1,...D/2-1
\end{shellcode}

There is no optimal policy in general as its performance depends on the node 
load and the node-to-node connection speed. It is worth trying both to check 
whether one is more performant than the other. The default policy is usually 
``by slot''(fill-first, usually preferred by non GPU-based softwares) but it
is always safer to explicitly set it for every run. 
In OpenMPI the corresponding options are \cmd{--byslot} and \cmd{--bynode}.

One final possibility is to run one process per device. This is the most 
consuming option from the point of view of the host memory, since every process 
will allocate on host the whole simulation scenario, but it might be useful if the 
GPUDirect feature is being used but the MPI runtime does not support multiple 
devices per process, or if the amount of data to be saved on file is large or 
the saving frequency is very high, so that saving would benefit from a parallel 
dump (every process saves its part of the simulation independently from the other processes).

Finally, let's see some practical examples. Suppose our cluster has 12 nodes, each equipped
with 4 GPU devices. We need to run a simulation on 12 devices. 
To run 3 processes on 3 hosts, each using 4 devices, we will run:

\begin{shellcode}
mpirun -np 3 -hostfile ./myhostsfile ./GPUSPH --device 0,1,2,3
\end{shellcode}

To run 6 processes on 6 hosts, each using 2 devices, we will run:

\begin{shellcode}
mpirun -np 6 -hostfile ./myhostsfile ./GPUSPH --device 0,1
\end{shellcode}

Note that another simulation can be run simultaneously on the remaining 2 devices of the same nodes, with:
\begin{shellcode}
mpirun -np 6 -hostfile ./myhostsfile ./GPUSPH --device 2,3
\end{shellcode}

To run 6 processes on 3 hosts, each using 2 devices, we will run:

\begin{shellcode}
mpirun -np 6 -hostfile ./myhostsfile_with3hosts \
--num-hosts 3 ./GPUSPH --device 0,1
\end{shellcode}

(--byslot-scheduling might also be necessary)

To run 12 processes on 12 hosts, each using 1 device, we will run:

\begin{shellcode}
mpirun -np 12 -hostfile ./myhostsfile ./GPUSPH --device 0
\end{shellcode}

Note that another simulation can be run simultaneously on one of the free devices of each node (e.g. device number 3), with:

\begin{shellcode}
mpirun -np 12 -hostfile ./myhostsfile ./GPUSPH --device 3
\end{shellcode}

Please note the options for the MPI library always precede the GPUSPH executable name.
If the MPI library supports it, we also suggest enabling the option to tag each 
line of the output with the process rank that has generated it; 
in OpenMPI, the option is called \cmd{--tag-output} while in MVAPICH \cmd{-prepend-rank}. 
This will come very helpful when the logs need to be analyzed (tip: use grep to separate the logs if they are multiplexed).

If you need to use any queue-management system, remember to inform it about the desired topology, 
coherently with the options passed to GPUSPH. For example, with PBS you would set the \cmd{nodes} 
and \cmd{ppn} parameters for the number of hosts and processes per host, respectively.


\section{Installing pre/post processing tools}

\subsection{Installing SALOME}
You can download SALOME from:
\url{http://www.salome-platform.org/downloads/current-version}

For this you need to register on the SALOME website.
Then, follow the installation instructions from the SALOME website and the
installer.

\textbf{Remark}: SALOME is used for SA boundary pre-processing, to generate an
STL mesh of the boundaries. You can of course use another mesher of your 
choice for this step.

\subsection{Installing CRIXUS}
Crixus is a preprocessing tool for GPUSPH.

\textbf{Prerequisites}:
\begin{itemize}
\item cmake $\ge$ 2.8
\item cuda
\item hdf5 $\ge$ 1.8.7
\end{itemize}

\textbf{Getting CRIXUS}:
\begin{shellcode}
git clone https://github.com/Azrael3000/Crixus.git
cd Crixus.git
\end{shellcode}

\textbf{Compiling Crixus}:
Crixus uses CMake for compilation. 
Let us assume that you have CRIXUS in a \cmd{Crixus.git} directory. 
and you want the building to happen in \cmd{Crixus.git/build} then follow the commands below:
\begin{shellcode}
mkdir build
cd build
cmake ..
make
\end{shellcode}
Note that you should not run cmake in the main Crixus folder.

The binary is then located at \cmd{Crixus.git/build/bin/Release/Crixus}.
Note that \cmd{make install} is not supported yet. To easily change the parameters of cmake you can use ccmake instead.

If hdf5 cannot be found due to lacking environmental variable you can edit the main CMakeLists.txt which has a commented line that reads:
\begin{shellcode}
#set(ENV{HDF5_ROOT} "/your/path/to/hdf5")
\end{shellcode}
Uncomment it and set the respective hdf5 path in order to use your custom installation.

To finish the installation it is recommended to add the path to the CRIXUS binary to your
\cmd{PATH} environment variable. Add this line in \cmd{~/.bashrc}:
\cmd{export PATH=/your_path/Crixus.git/build/bin/Release/Crixus:\$PATH}
where \cmd{/your_path} is your path to the CRIXUS directory.

\subsection{Installing PARAVIEW}

PARAVIEW is directly available from the Linux packages.

\newpage
\appendixpage
\appendix
\include{gpusph-license}

\bibliography{../gpusph-manual.bib}

\end{document}
